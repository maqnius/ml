\input{header}											% bindet Header ein

\renewcommand{\arraystretch}{1.2} % more whitespace in tables
\usepackage{array} % Being able to format columns of tables
\addbibresource{lit.bib} %Bibliographiedateien laden

\begin{document}
 
\title{Final Course Assignment \\ }%replace X with the appropriate number
\author{Mark Niehues, Stefaan Hessmann \\ %replace with your name
Mathematical Aspects in Machine Learning} %if necessary, replace with your course title

\maketitle
 
\section{Introduction}
In the past course we dealt with the broad mathematical foundations of machine learning. To get an idea of what the consequences of those mathematical theorems and approaches are and to get in touch with the standard Python tools, we have evaluated an comparatively easy data science example found on \url{kaggle.com}. Since this was our first machine learning project, we decided to deal with an rather simple problem. The example dataset \cite{Kaggle2017} consists of the historic passenger records of the disastrous Titanic maiden voyage in 1912. The goal in this challenge was to predict if a passenger survived the accident based on informations like for example age, sex and payed ticket fare. Therefore it is in terms of machine learning a \textit{classification problem} with two classes: survived and not survived.

Inspired by sample solutions from the website, we first took a deeper look on the dataset and tried to select the most significant influences by reviewing the statistical properties of the dataset. In the following we implemented an naive Sequential Minimal Optimization (SMO) algorithm and ran a few tests with them in order to finally compare the results with other machine learning algorithms.

\section{Applying Machine Learning Methods on the Titanic Disaster}
\subsection{Dataset}
The given dataset consists of a CSV-file containing data of 891 passengers. The dataset contains an ID for every passenger, a label if the passenger has survived the disaster and the features that are described in table \ref{tab:features}. It can be noticed that some of the features are incomplete.

\begin{table}
\caption{Features and their amount of missing data.}
\begin{tabular}{>{\bfseries}l l l}
PassengerId & Unique ID for every passenger & 0.0 \%\\
Survived & Survived (1) or died (0) & 0.0 \% \\
Pclass & Passenger's class & 0.0 \% \\
Name & Passenger's name & 0.0 \% \\
Sex & Passenger's sex & 0.0 \% \\
Age & Passenger's age & \textbf{19.87 \%} \\
SibSp & Number of siblings/spouses aboard & 0.0 \% \\
Parch & Number of parents/children aboard & 0.0 \% \\
Ticket & Ticket number & 0.0 \% \\
Fare & Ticket-price & 0.0 \% \\
Cabin & Number of the passenger's cabin & \textbf{77.10 \%} \\
Embarked & Port of embarkation & \textbf{0.22 \%} \\
\end{tabular}
\centering
\label{tab:features}
\end{table}

After loading the dataset, it is necessary to process the data for our learning machine. Therefore the different features will be investigated to select meaningful features and the missing data needs to be handled.

\subsection{Feature: Sex}
The sex-feature divides the passengers into the categories 'female' and 'male'. Figure \ref{fig:sexfeat} shows the probability of survival for male and female passengers. It is obvious that females had a much higher probability to survive than the male passengers. 'Sex' seems to be a useful feature for the learning machine.
 \begin{figure}
 	\centering
    \includegraphics[width=0.5\textwidth]{media_saved/sex_survived}
  \caption{Survival probability depending on the passenger's sex.}
  \label{fig:sexfeat}
 \end{figure}

\subsection{Feature: Age}
The survival distributions in function of the passengers' ages are pointed out in figure \ref{fig:agesexfeat}. The plot shows that children under twelve years were most likely to survive, whereas older children and young adults until about 30 years had bad chances. The chance for adults above their thirties to be rescued is nearly independent of their exact age at about 50 percent.\\ If we categorize the survival distribution depending on the age additionally by the sex feature as shown in figure \ref{fig:agesexfeat}, it turns out that the age has different effect on the survival probabilities of females and males. Young male passengers were likely to survive while males between about 12 and 30 years were unlikely to survive. This effect is inverted for females, where young girls had fewer chances to be rescued than females between about 25 and 40 years. From this it follows that age is a feature that can have influence on the predictions of our learning machine. Especially if it is combined with the sex feature. The missing values of the age dataset will be handled after the inspection of the other features.

 \begin{figure}

 \centering
     \includegraphics[width=0.5\textwidth]{media_saved/age_all}
     \caption{Survival probability depending on the passenger's age and sex.}
     \label{fig:agesexfeat}
     \includegraphics[width=0.5\textwidth]{media_saved/age_sex}
     \caption{Survival probability depending on the passenger's age and sex.}
     \label{fig:ageallfeat}
 \end{figure}
 
 \subsection{Feature: Name and Title}
The given dataset also contains a list of the passenger-names that have been involved in the titanic accident. At first sight a name does not appear to be a useful feature for our survival predictions, but the name-list contains also a persons title. Figure \ref{fig:title} shows the total number of occurrences for all titles that have been found. The titles 'Mr.', 'Master', 'Mrs.' and 'Miss' can are relatively common, whereas the other titles occur only infrequently. Therefore all other titles are grouped into a group named 'Res'. The figure also shows on the chances of survival for people with different titles on the right-handed plot. The plot points out that a persons title has an impact on the survival odds. It is obvious that there is a correlation between the title feature and the sex and the age feature.

 \begin{figure}
 \centering
     \includegraphics[width=0.5\textwidth]{media_saved/title}
     \caption{Total counts of the different titles (left) and survival probability in dependency of the title (right).}
     \label{fig:title}
 \end{figure}
 
 \subsection{Feature: Port of Embarkation}
 The titanic maiden voyage started in Southampton (S), but passengers could also be picked up in Cherbourg (C) and Queenstown (Q). The impact of the port of embarkation is displayed in figure \ref{fig:embarkationfeat}. The plot shows that passengers that joined the Titanic at Cherbourg survived relatively more often than people that embarked at other harbours. Against our expectations the port of embarkation seems to be a relevant feature for predictions about survival.
 
  \begin{figure}
 \centering
     \includegraphics[width=0.5\textwidth]{media_saved/embarkation_survived}
     \caption{Survival probability correlated with ports of embarkation.}
     \label{fig:embarkationfeat}
 \end{figure}
 
 \subsection{Feature: Family Constellation}
 The dataset contains two categories that handle family constellations. 'SibSp' contains the number of siblings and spouses a passenger was travelling with and 'Parch' contains the number of parents and children aboard. Figure \ref{fig:familyold} points out the influence of family constellations on the survival rate. The errorbars imply that the number of datasets with more than two siblings and spouses or more than two parents and children aboard is to sparse for reliable predictions. For this reason the two features are grouped to a new feature called 'Family' for every point in the dataset. This new feature holds the total number of a passengers relatives aboard which is grouped again into three categories containing people travelling alone, small families with one to three relatives aboard and big families with more than three relatives aboard. The classification into these groups was due to the similar survival rate of the number of relatives inside a class. The impact of the family feature is displayed in figure \ref{fig:familynew}. The the errorbars of the plot for the new feature are considerably smaller than before. In this way we can drop the 'SibSp' and the 'Parch' feature and replace it by the new family feature.
 
   \begin{figure}
 \centering
     \includegraphics[width=0.5\textwidth]{media_saved/family_size_before}
     \caption{Survival probability in dependency of the number of siblings / spouses (left) and parents / children (right) aboard.}
     \label{fig:familyold}
     \includegraphics[width=0.5\textwidth]{media_saved/family_size_after}
     \caption{Total number of datasets (left) survival rate (right) for the family feature.}
     \label{fig:familynew}
 \end{figure}
 
\subsection{Feature: Cabin Number}
The cabin number covers information about a passengers cabin position inside the Titanic. It can be expected that the cabin position correlates with the survival rate, because cabins at the upper decks were closer to the lifeboats than cabins further down the boat hull. Unfortunately about 80 \% of the cabin numbers were not preserved. In order to use the cabin number as a feature for the learning machine it is necessary to either find a way of recovering the missing data or to drop all datasets without a cabin number. Because the amount of missing data is so large, it is not probable to find a reliable way to recover the data and dropping 80 \% of the dataset is also not an option. If we reclassify the feature into 'cabin number preserved' and 'cabin number lost' it turns out that there is a significant difference between the survival rates of these two classes. This correlation is pointed out in figure \ref{fig:cabinfeat}.

\begin{figure}
 \centering
     \includegraphics[width=0.5\textwidth]{media_saved/cabin_survived}
     \caption{Correlation between preservation of cabin number and survival rate.}
     \label{fig:cabinfeat}
 \end{figure}

\subsection{Feature: Pclass and Fare}
The influence of prosperity on the survival rate is expressed through a passengers ticket price and the class of the cabin. The Influence of both features is correlated in figure \ref{fig:farefeat}. The fare feature was categorized into three groups of ticket prices with similar survival rates. It is obvious that rich passengers had better chances to get into a saving lifeboat than people in the cheaper classes. Both features will be used for predictions of the learning machine.

\subsection{Missing Data}
The features 'Cabin', 'Age' and 'Embarked' of our dataset are incomplete. In order to use these features for a learning machine one needs to handle the missing data. For the cabin feature this was already done by classifying the data into 'preserved' and 'lost'.\\
The embarked feature lacks under 1 \% of data so that these datasets can be dropped.\\


\begin{figure}
 \centering
     \includegraphics[width=0.5\textwidth]{media_saved/fare_survived}
     \caption{Correlation between Pclass (left), Fare (right) and survival rate.}
     \label{fig:farefeat}
 \end{figure}

\section{Implementation of an easy SMO Algorithm}
To get a better understanding of what a Support Vector Machine does, we decided to implement one on our own using several publications. Most of them were based on the important paper of Platt \cite{platt} where he introduced a new approach for the calculation of the Support Vectors that improves the performance a lot. This algorithm is called Sequential Minimal Optimization. Performance was not the highest priority for us but instead understandability and the costs of implementation. Therefore we implemented a less complex version of the algorithm presented in Platt's paper.

\section*{Appendix}
\appendix
\section{Pseudo Code of the SMO algorithm}
\begin{figure}[!h]
  \centering
    \includegraphics[width=0.8\textwidth]{media_saved/pseudo_code}
  \label{fig:gull}
\end{figure}

\section{Code}
\lstinputlisting[
	style=py,									% Style
	firstnumber={0},										% Start der Nummerierung
	firstline={15}											% 1. Codezeile
]											% letzte Codezeile
{./own_svm/kernels.py}

\printbibliography %hier Bibliographie ausgeben lassen
\end{document}        
