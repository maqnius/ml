\input{header}											% bindet Header ein

\renewcommand{\arraystretch}{1.2} % more whitespace in tables
\usepackage{array} % Being able to format columns of tables
\addbibresource{lit.bib} %Bibliographiedateien laden

\begin{document}
 
\title{Final Course Assignment \\ }%replace X with the appropriate number
\author{Mark Niehues, Stefaan Hessmann \\ %replace with your name
Mathematical Aspects in Machine Learning} %if necessary, replace with your course title

\maketitle
 
\section{Introduction}
In the past course we dealt with the broad mathematical foundations of machine learning. To get an idea of what the consequences of those mathematical theorems and approaches are and to get in touch with the standard Python tools, we have evaluated an comparatively easy data science example found on \url{kaggle.com}. Since this was our first machine learning project, we decided to deal with an rather simple problem. The example dataset \cite{Kaggle2017} consists of the historic passenger records of the disastrous Titanic maiden voyage in 1912. The goal in this Challenge was to predict if a passenger survived the accident based on informations like for example age, sex and payed ticket fare. Therefore it is in terms of machine learning a \textit{classification problem} with two classes: survived and not survived.

Inspired by sample solutions from the website, we first took a deeper look on the dataset and tried to select the most significant influences by reviewing the statistical properties of the dataset. In the following we implemented an naive Sequential Minimal Optimization (SMO) algorithm and ran a few tests with them in order to finally compare the results with other machine learning algorithms.

\section{Applying Machine Learning Methods on the Titanic Disaster}
\subsection{Dataset}
The given dataset consists of a CSV-file containing data of 891 passengers. The dataset contains an ID for every passenger, a label if the passenger has survived the disaster and the features that are described in table \ref{tab:features}. It can be noticed that some of the features are incomplete.

\begin{table}
\caption{Features and their amount of missing data.}
\begin{tabular}{>{\bfseries}l l l}
PassengerId & Unique ID for every passenger & 0.0 \%\\
Survived & Survived (1) or died (0) & 0.0 \% \\
Pclass & Passenger's class & 0.0 \% \\
Name & Passenger's name & 0.0 \% \\
Sex & Passenger's sex & 0.0 \% \\
Age & Passenger's age & \textbf{19.87 \%} \\
SibSp & Number of siblings/spouses aboard & 0.0 \% \\
Parch & Number of parents/children aboard & 0.0 \% \\
Ticket & Ticket number & 0.0 \% \\
Fare & Ticket-price & 0.0 \% \\
Cabin & Number of the passenger's cabin & \textbf{77.10 \%} \\
Embarked & Port of embarkation & \textbf{0.22 \%} \\
\end{tabular}
\centering
\label{tab:features}
\end{table}

After loading the dataset it is necessary to process the data for our learning machine. Therefore the different features will be investigated to select meaningful features and the missing data needs to be handled.

\subsection{Feature: Sex}
The sex-feature divides the passengers into the categories 'female' and 'male'. Figure \ref{fig:sexfeat} shows the probability of survival for male and female passengers. It is obvious that females had a much higher probability to survive than the male passengers. 'Sex' seems to be a useful feature for the learning machine.
 \begin{figure}
 	\centering
    \includegraphics[width=0.5\textwidth]{media_saved/sex_survived}
  \caption{Survival probability depending on the passenger's sex.}
  \label{fig:sexfeat}
 \end{figure}

\subsection{Feature: Age}
The impact of a passengers' age on their probability to survive categorized by their sex is pointed out 

\section{Implementation of an easy SMO Algorithm}
To get a better understanding of what a Support Vector Machine does we decided to implement one on our own using several publications. Most of them were based on the important paper of Platt \cite{platt} where he introduced a new approach for the calculation of the Support Vectors that improves the performance a lot. This algorithm is called Sequential Minimal Optimization. Performance was not the highest priority for us but instead understandability and the costs of implementation. Therefore we implemented a less complex version of the algorithm presented in Platt's paper.

As the mathematical background of the SVMs has been explained in the lecture and might be considered a standard solution for machine learning, the following introduction focuses on the main equations.

The initial problem is a linear separable dataset with the labels $y_i \in \{-1, 1\}$. The classifier that the SVM is supposed to compute will have the form

\begin{equation}
    f(x) = <\omega, x> + b
\end{equation}

Now suppose we have a separating hyperplane and $w$ is perpendicular. The main task of the SVM is to maximise the closest perpendicular distance between the hyperplane and the two classes. This is down by the following constraints

\begin{align}
f(x) &\geq 1\ \text{for $y_i = +1$} \\
f(x) &\leq 1\ \text{for $y_i = -1$}
\end{align}

Consequently do points that lie on the hyperplane satisfy $f(x)=0$.

From these set of equations follows that the minimal distance from the hyperplane to one of the datapoints is $d=\frac{1}{|\omega|}$ which shall be maximized. Introducing an additional factor that allows but penalizes non separable noise and reformulating the problem with Lagrange multipliers (the $\alpha _i$) we get the following problem:

\begin{align}
\text{max}_\alpha& W(\alpha) = \sum_{i=1}^m \alpha _i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m y^{(i)} y^{(j)} \alpha _i <x^{(i)}, x^{(j)}> \\
\text{subject to \em} & 0\leq \alpha _i \leq C, i = 1,..., m \\
& \sum_{i=1}^m \alpha _i y^{(i)} = 0
\end{align}

For the presented problem the Kuhn Tucker conditions define the $\alpha _i$ that represent an optimal solution. The KKT conditions are

\begin{align}
\alpha _i = 0 & \implies  y^{(i)}(<\omega, x^{(i)}> + b)  \geq 1 \\
\alpha _i = C & \implies  y^{(i)}(<\omega, x^{(i)}> + b)  \leq 1 \\
0 \geq \alpha _i \geq C & \implies  y^{(i)}(<\omega, x^{(i)}> + b)  = 1 
\end{align}


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{media_saved/own_test_mpasses_1.pdf}
        \caption{\texttt{max\_passes=1}}
        \label{fig:max_passs_1}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{media_saved/own_test_mpasses_15.pdf}
        \caption{\texttt{max\_passes=15}}
        \label{fig:max_pass_15}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{media_saved/own_test_mpasses_30.pdf}
        \caption{\texttt{max\_passes=30}}
        \label{fig:max_pass_30}
    \end{subfigure}
    \caption{Result of our SVM depending on number of loops without any chaning $\alpha$ that terminate the alogrithm.}\label{fig:max_passes}
\end{figure}

\section*{Appendix}
\appendix
\section{Pseudo Code of the SMO algorithm}
\begin{figure}[!h]
  \centering
    \includegraphics[width=0.8\textwidth]{media_saved/pseudo_code}
  \label{fig:gull}
\end{figure}

\section{Code}
\lstinputlisting[
	style=py,									% Style
	firstnumber={0},										% Start der Nummerierung
	firstline={15}											% 1. Codezeile
]											% letzte Codezeile
{./own_svm/kernels.py}

\printbibliography %hier Bibliographie ausgeben lassen
\end{document}        
