\input{header}											% bindet Header ein (WICHTIG)

\setsansfont{Source Sans Pro}
\setmainfont{Source Sans Pro}

\usepackage[babel, german=quotes]{csquotes} % einfache Handhabung von quotations
\usepackage[backend=biber]{biblatex} %biblatex mit biber laden
\ExecuteBibliographyOptions{
sorting=nyt, %Sortierung Autor, Titel, Jahr
bibwarn=true, %Probleme mit den Daten, die Backend betreffen anzeigen
isbn=false, %keine isbn anzeigen
url=false %keine url anzeigen
}

\addbibresource{lit.bib} %Bibliographiedateien laden

\begin{document}
 
\title{Final Course Assignment \\ }%replace X with the appropriate number
\author{Mark Niehues, Stefaan Hessmann \\ %replace with your name
Mathematical Aspects in Machine Learning} %if necessary, replace with your course title

\maketitle
 
\section{Introduction}
In the past course we dealt with the broad mathematical foundations of machine learning. To get an idea of what the consequences of those mathematical theorems and approaches are and to get in touch with the standard Python tools, we have evaluated an comparatively easy data science example found on \url{kaggle.com}. The example dataset \cite{Kaggle2017} consists of the historic passenger records of the disastrous Titanic maiden voyage in 1912. 

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{media_saved/benchmark}
  \caption{Benchmark}
  \label{fig:gull}
\end{figure}

\lstinputlisting[
	style=py,									% Style
	caption={Hello World},		% Beschriftung
	firstnumber={0},										% Start der Nummerierung
	firstline={0}											% 1. Codezeile
]											% letzte Codezeile
{./own_svm/kernels.py}


\section{Applying Machine Learning Methods on the Titanic Disaster}
\subsection{Dataset}
The given dataset consists of a CSV-file containing data of 891 passengers. The dataset contains an ID for every passenger, a label if the passenger has survived the disaster and the features that are described in table \ref{tab:features}. It can be noticed that some of the features are incomplete.

\begin{table}
\begin{tabular}{|l|l|c|}
\hline
Feature & Description & Missing Data (\%) \\ \hline
PassengerId & Unique ID for every passenger & 0.0 \\ \hline
Survived & Survived (1) or died (0) & 0.0 \\ \hline
Pclass & Passenger's class & 0.0 \\ \hline
Name & Passenger's name & 0.0 \\ \hline
Sex & Passenger's sex & 0.0 \\ \hline
Age & Passenger's age & 19.87 \\ \hline
SibSp & Number of siblings/spouses aboard & 0.0 \\ \hline
Parch & Number of parents/children aboard & 0.0 \\ \hline
Ticket & Ticket number & 0.0 \\ \hline
Fare & Ticket-price & 0.0 \\ \hline
Cabin & Number of the passenger's cabin & 77.10 \\ \hline
Embarked & Port of embarkation & 0.22\\ \hline
\end{tabular}
\centering
\label{tab:features}
\caption{Description of the dataset.}
\end{table}

After loading the dataset, it is necessary to process the data for our learning machine. Therefore the different features will be investigated to select meaningful features and the missing data needs to be handled.

\subsection{Feature: Sex}
The sex-feature divides the passengers into the categories 'female' and 'male'. Figure \ref{fig:sexfeat} shows the probability of survival for male and female passengers. It is obvious that females had a much higher probability to survive than the male passengers. 'Sex' seems to be a useful feature for the learning machine.
 \begin{figure}
 	\centering
    \includegraphics[width=0.5\textwidth]{media_saved/sex_survived}
  \caption{Survival probability depending on the passenger's sex.}
  \label{fig:sexfeat}
 \end{figure}

\subsection{Feature: Age}
The impact of a passengers' age on their probability to survive categorized by their sex is pointed out 

\section{Implementation of an easy SMO Algorithm}

\printbibliography %hier Bibliographie ausgeben lassen
\end{document}        